{"BzpIiHogiW": [{"name": "stderr", "output_type": "stream", "text": ["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c636d5191dae447b8383f55e6898137e", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading shards:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"ename": "RuntimeError", "evalue": "Error(s) in loading state_dict for PhiForCausalLM:\n\tsize mismatch for model.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.10.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.10.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.10.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.10.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.11.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.11.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.11.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.11.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.12.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.12.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.12.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.12.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.13.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.13.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.13.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.13.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.14.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.14.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.14.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.14.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.15.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.15.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.15.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.15.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.16.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.16.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.16.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.16.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.17.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.17.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.17.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.17.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.18.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.18.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.18.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.18.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.19.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.19.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.19.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.19.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.final_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.final_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([51200, 2560]) from checkpoint, the shape in current model is torch.Size([51200, 2048]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)", "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mPhudish/Test_Tam_2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Load the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mPhudish/Test_Tam_2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# def ask(tasktext):\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# input_text = \u201cgenerate: Write a poem about Ireland\u201d\u001b[39;00m\n\u001b[1;32m      8\u001b[0m input_text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mquestion: what is 2+2?\u001b[39m\u001b[39m\"\u001b[39m\n", "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n", "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:3850\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3841\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3842\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3843\u001b[0m     (\n\u001b[1;32m   3844\u001b[0m         model,\n\u001b[1;32m   3845\u001b[0m         missing_keys,\n\u001b[1;32m   3846\u001b[0m         unexpected_keys,\n\u001b[1;32m   3847\u001b[0m         mismatched_keys,\n\u001b[1;32m   3848\u001b[0m         offload_index,\n\u001b[1;32m   3849\u001b[0m         error_msgs,\n\u001b[0;32m-> 3850\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3851\u001b[0m         model,\n\u001b[1;32m   3852\u001b[0m         state_dict,\n\u001b[1;32m   3853\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3854\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3855\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3856\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3857\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3858\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3859\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3860\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3861\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3862\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3863\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3864\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3865\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3866\u001b[0m     )\n\u001b[1;32m   3868\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3869\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n", "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:4335\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4331\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msize mismatch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_msg:\n\u001b[1;32m   4332\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m   4333\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4334\u001b[0m         )\n\u001b[0;32m-> 4335\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4337\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unexpected_keys) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   4338\u001b[0m     archs \u001b[39m=\u001b[39m [] \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39marchitectures \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39marchitectures\n", "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PhiForCausalLM:\n\tsize mismatch for model.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.10.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.10.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.10.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.10.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.10.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.10.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.10.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.10.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.11.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.11.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.11.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.11.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.11.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.11.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.11.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.11.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.12.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.12.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.12.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.12.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.12.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.12.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.12.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.12.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.13.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.13.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.13.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.13.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.13.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.13.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.13.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.13.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.14.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.14.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.14.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.14.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.14.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.14.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.14.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.14.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.15.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.15.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.15.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.15.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.15.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.15.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.15.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.15.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.16.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.16.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.16.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.16.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.16.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.16.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.16.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.16.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.17.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.17.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.17.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.17.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.17.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.17.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.17.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.17.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.18.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.18.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.18.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.18.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.18.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.18.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.18.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.18.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.19.self_attn.q_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.19.self_attn.k_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.19.self_attn.v_proj.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.self_attn.dense.weight: copying a param with shape torch.Size([2560, 2560]) from checkpoint, the shape in current model is torch.Size([2048, 2048]).\n\tsize mismatch for model.layers.19.self_attn.dense.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.mlp.fc1.weight: copying a param with shape torch.Size([10240, 2560]) from checkpoint, the shape in current model is torch.Size([8192, 2048]).\n\tsize mismatch for model.layers.19.mlp.fc1.bias: copying a param with shape torch.Size([10240]) from checkpoint, the shape in current model is torch.Size([8192]).\n\tsize mismatch for model.layers.19.mlp.fc2.weight: copying a param with shape torch.Size([2560, 10240]) from checkpoint, the shape in current model is torch.Size([2048, 8192]).\n\tsize mismatch for model.layers.19.mlp.fc2.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.input_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.layers.19.input_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.final_layernorm.weight: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for model.final_layernorm.bias: copying a param with shape torch.Size([2560]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([51200, 2560]) from checkpoint, the shape in current model is torch.Size([51200, 2048]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."]}]}